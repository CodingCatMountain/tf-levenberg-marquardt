{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "levenberg_marquardt.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "FEf5CbTCVa5p"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEf5CbTCVa5p"
      },
      "source": [
        "# Levenberg-Marquardt\n",
        "\n",
        "Implementation of Levenberg-Marquardt training for models that inherits from `tf.keras.Model`. The algorithm has been extended to support **mini-batch** training for both **regression** and **classification** problems."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jphX--iNNB5I",
        "cellView": "both"
      },
      "source": [
        "# Copyright (c) 2020 Fabio Di Marco\n",
        "# \n",
        "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "# of this software and associated documentation files (the \"Software\"), to deal\n",
        "# in the Software without restriction, including without limitation the rights\n",
        "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "# copies of the Software, and to permit persons to whom the Software is\n",
        "# furnished to do so, subject to the following conditions:\n",
        "# \n",
        "# The above copyright notice and this permission notice shall be included in all\n",
        "# copies or substantial portions of the Software.\n",
        "# \n",
        "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
        "# SOFTWARE.\n",
        "# ==============================================================================\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# ==============================================================================\n",
        "\n",
        "\n",
        "class MeanSquaredError(tf.keras.losses.MeanSquaredError):\n",
        "    \"\"\"Provides mean squared error metrics: loss / residuals.\n",
        "\n",
        "    Use mean squared error for regression problems with one or more outputs.\n",
        "    \"\"\"\n",
        "\n",
        "    @tf.function\n",
        "    def residuals(self, y_true, y_pred):\n",
        "        return y_true - y_pred\n",
        "\n",
        "\n",
        "class ReducedOutputsMeanSquaredError(tf.keras.losses.Loss):\n",
        "    \"\"\"Provides mean squared error metrics: loss / residuals.\n",
        "\n",
        "    Consider using this reduced outputs mean squared error loss for regression\n",
        "    problems with a large number of outputs or at least more then one output.\n",
        "    This loss function reduces the number of outputs from N to 1, reducing both\n",
        "    the size of the jacobian matrix and backpropagation complexity.\n",
        "    Tensorflow, in fact, uses backward differentiation which computational\n",
        "    complexity is  proportional to the number of outputs.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 reduction=tf.keras.losses.Reduction.AUTO,\n",
        "                 name='reduced_outputs_mean_squared_error'):\n",
        "        super(ReducedOutputsMeanSquaredError, self).__init__(\n",
        "            reduction=reduction,\n",
        "            name=name)\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, y_true, y_pred):\n",
        "        sq_diff = tf.math.squared_difference(y_true, y_pred)\n",
        "        return tf.math.reduce_mean(sq_diff, axis=1)\n",
        "\n",
        "    @tf.function\n",
        "    def residuals(self, y_true, y_pred):\n",
        "        sq_diff = tf.math.squared_difference(y_true, y_pred)\n",
        "        eps = tf.keras.backend.epsilon()\n",
        "        return tf.math.sqrt(eps + tf.math.reduce_mean(sq_diff, axis=1))\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "    The gauss-newthon algorithm is obtained from the linear approximation of the\n",
        "    squared residuals and it is used solve least square problems.\n",
        "    A way to use cross-entropy instead of mean squared error is to compute\n",
        "    residuals as the square root of the cross-entropy.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class CategoricalCrossentropy(tf.keras.losses.CategoricalCrossentropy):\n",
        "    \"\"\"Provides cross-entropy metrics: loss / residuals.\n",
        "\n",
        "    Use this cross-entropy loss for classification problems with two or more\n",
        "    label classes. The labels are expected to be provided in a `one_hot`\n",
        "    representation.\n",
        "    \"\"\"\n",
        "\n",
        "    @tf.function\n",
        "    def residuals(self, y_true, y_pred):\n",
        "        eps = tf.keras.backend.epsilon()\n",
        "        return tf.math.sqrt(eps + self.fn(y_true, y_pred, **self._fn_kwargs))\n",
        "\n",
        "\n",
        "class SparseCategoricalCrossentropy(\n",
        "        tf.keras.losses.SparseCategoricalCrossentropy):\n",
        "    \"\"\"Provides cross-entropy metrics: loss / residuals.\n",
        "\n",
        "    Use this cross-entropy loss for classification problems with two or more\n",
        "    label classes. The labels are expected to be provided as integers.\n",
        "    \"\"\"\n",
        "\n",
        "    @tf.function\n",
        "    def residuals(self, y_true, y_pred):\n",
        "        eps = tf.keras.backend.epsilon()\n",
        "        return tf.math.sqrt(eps + self.fn(y_true, y_pred, **self._fn_kwargs))\n",
        "\n",
        "\n",
        "class BinaryCrossentropy(tf.keras.losses.BinaryCrossentropy):\n",
        "    \"\"\"Provides cross-entropy metrics: loss / residuals.\n",
        "\n",
        "    Use this cross-entropy loss for classification problems with only two label\n",
        "    classes (assumed to be 0 and 1). For each example, there should be a single\n",
        "    floating-point value per prediction.\n",
        "    \"\"\"\n",
        "\n",
        "    @tf.function\n",
        "    def residuals(self, y_true, y_pred):\n",
        "        eps = tf.keras.backend.epsilon()\n",
        "        return tf.math.sqrt(eps + self.fn(y_true, y_pred, **self._fn_kwargs))\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "    Other experimental losses for classification problems.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class SquaredCategoricalCrossentropy(tf.keras.losses.Loss):\n",
        "    \"\"\"Provides squared cross-entropy metrics: loss / residuals.\n",
        "\n",
        "    Use this cross-entropy loss for classification problems with two or more\n",
        "    label classes. The labels are expected to be provided in a `one_hot`\n",
        "    representation.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 from_logits=False,\n",
        "                 label_smoothing=0,\n",
        "                 reduction=tf.keras.losses.Reduction.AUTO,\n",
        "                 name='squared_categorical_crossentropy'):\n",
        "        super(SquaredCategoricalCrossentropy, self).__init__(\n",
        "            reduction=reduction,\n",
        "            name=name)\n",
        "        self.from_logits = from_logits\n",
        "        self.label_smoothing = label_smoothing\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, y_true, y_pred):\n",
        "        return tf.math.square(tf.keras.losses.categorical_crossentropy(\n",
        "            y_true,\n",
        "            y_pred,\n",
        "            self.from_logits,\n",
        "            self.label_smoothing))\n",
        "\n",
        "    @tf.function\n",
        "    def residuals(self, y_true, y_pred):\n",
        "        return tf.keras.losses.categorical_crossentropy(\n",
        "            y_true,\n",
        "            y_pred,\n",
        "            self.from_logits,\n",
        "            self.label_smoothing)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {'from_logits': self.from_logits,\n",
        "                  'label_smoothing': self.label_smoothing}\n",
        "        base_config = super(SquaredCategoricalCrossentropy, self).get_config()\n",
        "        return dict(base_config + config)\n",
        "\n",
        "\n",
        "class CategoricalMeanSquaredError(tf.keras.losses.Loss):\n",
        "    \"\"\"Provides mean squared error metrics: loss / residuals.\n",
        "\n",
        "    Use this categorical mean squared error loss for classification problems\n",
        "    with two or more label classes. The labels are expected to be provided in a\n",
        "    `one_hot` representation and the output activation to be softmax.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 reduction=tf.keras.losses.Reduction.AUTO,\n",
        "                 name='categorical_mean_squared_error'):\n",
        "        super(CategoricalMeanSquaredError, self).__init__(\n",
        "            reduction=reduction,\n",
        "            name=name)\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, y_true, y_pred):\n",
        "        # Selects the y_pred which corresponds to y_true equal to 1.\n",
        "        prediction = tf.reduce_sum(tf.math.multiply(y_true, y_pred), axis=1)\n",
        "        return tf.math.squared_difference(1.0, prediction)\n",
        "\n",
        "    @tf.function\n",
        "    def residuals(self, y_true, y_pred):\n",
        "        # Selects the y_pred which corresponds to y_true equal to 1.\n",
        "        prediction = tf.reduce_sum(tf.math.multiply(y_true, y_pred), axis=1)\n",
        "        return 1.0 - prediction\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "\n",
        "\n",
        "class DampingAlgorithm:\n",
        "    \"\"\"Default Levenberg–Marquardt damping algorithm.\n",
        "\n",
        "    This is used inside the Trainer as a generic class. Many damping algorithms\n",
        "    can be implemented using the same interface.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 starting_value=1e-3,\n",
        "                 dec_factor=0.1,\n",
        "                 inc_factor=10.0,\n",
        "                 min_value=1e-10,\n",
        "                 max_value=1e+10,\n",
        "                 fletcher=False):\n",
        "        \"\"\"Initializes `DampingAlgorithm` instance.\n",
        "\n",
        "        Args:\n",
        "          starting_value: (Optional) Used to initialize the Trainer internal\n",
        "            damping_factor.\n",
        "          dec_factor: (Optional) Used in the train_step decrease the\n",
        "            damping_factor when new_loss < loss.\n",
        "          inc_factor: (Optional) Used in the train_step increase the\n",
        "            damping_factor when new_loss >= loss.\n",
        "          min_value: (Optional) Used as a lower bound for the damping_factor.\n",
        "            Higher values improve numerical stability in the resolution of the\n",
        "            linear system, at the cost of slower convergence.\n",
        "          max_value: (Optional) Used as an upper bound for the damping_factor,\n",
        "            and as condition to stop the Training process.\n",
        "          fletcher: Bool (Optional) Replace the identity matrix with\n",
        "            diagonal of the gauss-newton hessian approximation, so that there is\n",
        "            larger movement along the directions where the gradient is smaller.\n",
        "            This avoids slow convergence in the direction of small gradient.\n",
        "        \"\"\"\n",
        "        self.starting_value = starting_value\n",
        "        self.dec_factor = dec_factor\n",
        "        self.inc_factor = inc_factor\n",
        "        self.min_value = min_value\n",
        "        self.max_value = max_value\n",
        "        self.fletcher = fletcher\n",
        "\n",
        "    @tf.function\n",
        "    def init_step(self, damping_factor, loss):\n",
        "        return damping_factor\n",
        "\n",
        "    @tf.function\n",
        "    def decrease(self, damping_factor, loss):\n",
        "        return tf.math.maximum(\n",
        "            damping_factor * self.dec_factor,\n",
        "            self.min_value)\n",
        "\n",
        "    @tf.function\n",
        "    def increase(self, damping_factor, loss):\n",
        "        return tf.math.minimum(\n",
        "            damping_factor * self.inc_factor,\n",
        "            self.max_value)\n",
        "\n",
        "    @tf.function\n",
        "    def stop_training(self, damping_factor, loss):\n",
        "        return damping_factor >= self.max_value\n",
        "\n",
        "    @tf.function\n",
        "    def apply(self, damping_factor, JJ):\n",
        "        if self.fletcher:\n",
        "            damping = tf.linalg.tensor_diag(tf.linalg.diag_part(JJ))\n",
        "        else:\n",
        "            damping = tf.eye(tf.shape(JJ)[0], dtype=JJ.dtype)\n",
        "\n",
        "        damping = tf.scalar_mul(damping_factor, damping)\n",
        "        return tf.add(JJ, damping)\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "\n",
        "class Trainer:\n",
        "    \"\"\"Levenberg–Marquardt training algorithm.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 model,\n",
        "                 optimizer=tf.keras.optimizers.SGD(learning_rate=1.0),\n",
        "                 loss=MeanSquaredError(),\n",
        "                 damping_algorithm=DampingAlgorithm(),\n",
        "                 attempts_per_step=10,\n",
        "                 solve_method='solve',\n",
        "                 jacobian_max_num_rows=100,\n",
        "                 experimental_use_pfor=True):\n",
        "        \"\"\"Initializes `Trainer` instance.\n",
        "\n",
        "        Args:\n",
        "          model: It is the Model to be trained, it is expected to inherit\n",
        "            from tf.keras.Model and to be already built.\n",
        "          optimizer: (Optional) Performs the update of the model trainable\n",
        "            variables. When tf.keras.optimizers.SGD is used it is equivalent\n",
        "            to the operation `w = w - learning_rate * updates`, where updates is\n",
        "            the step computed using the Levenberg-Marquardt algorithm.\n",
        "          loss: (Optional) An object which inherits from tf.keras.losses.Loss\n",
        "          and have an additional function to compute residuals.\n",
        "          damping_algorithm: (Optional) Class implementing the damping\n",
        "            algorithm to use during training.\n",
        "          attempts_per_step: Integer (Optional) During the train step when new\n",
        "            model variables are computed, the new loss is evaluated and compared\n",
        "            with the old loss value. If new_loss < loss, then the new variables\n",
        "            are accepted, otherwise the old variables are restored and\n",
        "            new ones are computed using a different damping-factor.\n",
        "            This argument represents the maximum number of attempts, after which\n",
        "            the step is taken.\n",
        "          solve_method: (Optional) Possible values are:\n",
        "            'qr': Uses QR decomposition which is robust but slower.\n",
        "            'cholesky': Uses Cholesky decomposition which is fast but may fail\n",
        "                when the hessian approximation is ill-conditioned.\n",
        "            'solve': Uses tf.linalg.solve. I don't know what algorithm it\n",
        "                implements. But it seems a compromise in terms of speed and\n",
        "                robustness.\n",
        "          jacobian_max_num_rows: Integer (Optional) When the number of residuals\n",
        "            is greater then the number of variables (overdetermined), the\n",
        "            hessian approximation is computed by slicing the input and\n",
        "            accumulate the result of each computation. In this way it is\n",
        "            possible to drastically reduce the memory usage and increase the\n",
        "            speed as well. The input is sliced into blocks of size less than or\n",
        "            equal to the jacobian_max_num_rows.\n",
        "          experimental_use_pfor: (Optional) If true, vectorizes the jacobian\n",
        "            computation. Else falls back to a sequential while_loop.\n",
        "            Vectorization can sometimes fail or lead to excessive memory usage.\n",
        "            This option can be used to disable vectorization in such cases.\n",
        "        \"\"\"\n",
        "        if not model.built:\n",
        "            raise ValueError('Trainer model has not yet been built. '\n",
        "                             'Build the model first by calling `build()` or '\n",
        "                             'calling `fit()` with some data, or specify an '\n",
        "                             '`input_shape` argument in the first layer(s) for '\n",
        "                             'automatic build.')\n",
        "\n",
        "        self.model = model\n",
        "        self.loss = loss\n",
        "        self.optimizer = optimizer\n",
        "        self.damping_algorithm = damping_algorithm\n",
        "        self.attempts_per_step = attempts_per_step\n",
        "        self.jacobian_max_num_rows = jacobian_max_num_rows\n",
        "        self.experimental_use_pfor = experimental_use_pfor\n",
        "\n",
        "        # Define and select linear system equation solver.\n",
        "        def qr(matrix, rhs):\n",
        "            q, r = tf.linalg.qr(matrix, full_matrices=True)\n",
        "            y = tf.linalg.matmul(q, rhs, transpose_a=True)\n",
        "            return tf.linalg.triangular_solve(r, y, lower=False)\n",
        "\n",
        "        def cholesky(matrix, rhs):\n",
        "            chol = tf.linalg.cholesky(matrix)\n",
        "            return tf.linalg.cholesky_solve(chol, rhs)\n",
        "\n",
        "        def solve(matrix, rhs):\n",
        "            return tf.linalg.solve(matrix, rhs)\n",
        "\n",
        "        if solve_method == 'qr':\n",
        "            self.solve_function = qr\n",
        "        elif solve_method == 'cholesky':\n",
        "            self.solve_function = cholesky\n",
        "        elif solve_method == 'solve':\n",
        "            self.solve_function = solve\n",
        "        else:\n",
        "            raise ValueError('Invalid solve_method.')\n",
        "\n",
        "        # Keep track of the current damping_factor.\n",
        "        self.damping_factor = tf.Variable(\n",
        "            self.damping_algorithm.starting_value,\n",
        "            trainable=False,\n",
        "            dtype=self.model.dtype)\n",
        "\n",
        "        # Used to backup and restore model variables.\n",
        "        self._backup_variables = []\n",
        "\n",
        "        # Since training updates are computed with shape (num_variables, 1),\n",
        "        # self._splits and self._shapes are needed to split and reshape the\n",
        "        # updates so that they can be applied to the model trainable_variables.\n",
        "        self._splits = []\n",
        "        self._shapes = []\n",
        "\n",
        "        for variable in self.model.trainable_variables:\n",
        "            variable_shape = tf.shape(variable)\n",
        "            variable_size = tf.reduce_prod(variable_shape)\n",
        "            backup_variable = tf.Variable(\n",
        "                tf.zeros_like(variable),\n",
        "                trainable=False)\n",
        "\n",
        "            self._backup_variables.append(backup_variable)\n",
        "            self._splits.append(variable_size)\n",
        "            self._shapes.append(variable_shape)\n",
        "\n",
        "        self._num_variables = tf.reduce_sum(self._splits).numpy().item()\n",
        "        self._num_outputs = None\n",
        "\n",
        "    @tf.function\n",
        "    def _compute_jacobian(self, inputs, targets):\n",
        "        with tf.GradientTape(persistent=True) as tape:\n",
        "            outputs = self.model(inputs, training=True)\n",
        "            residuals = self.loss.residuals(targets, outputs)\n",
        "\n",
        "        jacobians = tape.jacobian(\n",
        "            residuals,\n",
        "            self.model.trainable_variables,\n",
        "            experimental_use_pfor=self.experimental_use_pfor)\n",
        "\n",
        "        del tape\n",
        "\n",
        "        num_residuals = tf.reduce_prod(tf.shape(residuals))\n",
        "        jacobians = [tf.reshape(j, (num_residuals, -1)) for j in jacobians]\n",
        "        jacobian = tf.concat(jacobians, axis=1)\n",
        "        residuals = tf.reshape(residuals, (num_residuals, -1))\n",
        "\n",
        "        return jacobian, residuals, outputs\n",
        "\n",
        "    @tf.function\n",
        "    def _init_gauss_newton_overdetermined(self, inputs, targets):\n",
        "        # Perform the following computation:\n",
        "        # J, residuals, outputs = self._compute_jacobian(inputs, targets)\n",
        "        # JJ = tf.linalg.matmul(J, J, transpose_a=True)\n",
        "        # rhs = tf.linalg.matmul(J, residuals, transpose_a=True)\n",
        "        #\n",
        "        # But reduce memory usage by slicing the inputs so that the jacobian\n",
        "        # matrix will have maximum shape (jacobian_max_num_rows, num_variables)\n",
        "        # instead of (batch_size, num_variables).\n",
        "        slice_size = self.jacobian_max_num_rows // self._num_outputs\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        num_slices = batch_size // slice_size\n",
        "        remainder = batch_size % slice_size\n",
        "\n",
        "        JJ = tf.zeros(\n",
        "            [self._num_variables, self._num_variables],\n",
        "            dtype=self.model.dtype)\n",
        "\n",
        "        rhs = tf.zeros(\n",
        "            [self._num_variables, 1],\n",
        "            dtype=self.model.dtype)\n",
        "\n",
        "        outputs_array = tf.TensorArray(tf.float32, size=0, dynamic_size=True)\n",
        "\n",
        "        for i in tf.range(num_slices):\n",
        "            tf.autograph.experimental.set_loop_options(\n",
        "                shape_invariants=[\n",
        "                    (rhs, tf.TensorShape((self._num_variables, None)))])\n",
        "\n",
        "            _inputs = inputs[i * slice_size:(i + 1) * slice_size]\n",
        "            _targets = targets[i * slice_size:(i + 1) * slice_size]\n",
        "\n",
        "            J, residuals, _outputs = self._compute_jacobian(_inputs, _targets)\n",
        "\n",
        "            outputs_array = outputs_array.write(i, _outputs)\n",
        "\n",
        "            JJ += tf.linalg.matmul(J, J, transpose_a=True)\n",
        "            rhs += tf.linalg.matmul(J, residuals, transpose_a=True)\n",
        "\n",
        "        if remainder > 0:\n",
        "            _inputs = inputs[num_slices * slice_size::]\n",
        "            _targets = targets[num_slices * slice_size::]\n",
        "\n",
        "            J, residuals, _outputs = self._compute_jacobian(_inputs, _targets)\n",
        "\n",
        "            if num_slices > 0:\n",
        "                outputs = tf.concat([outputs_array.concat(), _outputs], axis=0)\n",
        "            else:\n",
        "                outputs = _outputs\n",
        "\n",
        "            JJ += tf.linalg.matmul(J, J, transpose_a=True)\n",
        "            rhs += tf.linalg.matmul(J, residuals, transpose_a=True)\n",
        "        else:\n",
        "            outputs = outputs_array.concat()\n",
        "\n",
        "        return 0.0, JJ, rhs, outputs\n",
        "\n",
        "    @tf.function\n",
        "    def _init_gauss_newton_underdetermined(self, inputs, targets):\n",
        "        J, residuals, outputs = self._compute_jacobian(inputs, targets)\n",
        "        JJ = tf.linalg.matmul(J, J, transpose_b=True)\n",
        "        rhs = residuals\n",
        "        return J, JJ, rhs, outputs\n",
        "\n",
        "    @tf.function\n",
        "    def _compute_gauss_newton_overdetermined(self, J, JJ, rhs):\n",
        "        updates = self.solve_function(JJ, rhs)\n",
        "        return updates\n",
        "\n",
        "    @tf.function\n",
        "    def _compute_gauss_newton_underdetermined(self, J, JJ, rhs):\n",
        "        updates = self.solve_function(JJ, rhs)\n",
        "        updates = tf.linalg.matmul(J, updates, transpose_a=True)\n",
        "        return updates\n",
        "\n",
        "    @tf.function\n",
        "    def _train_step(self, inputs, targets,\n",
        "                    init_gauss_newton, compute_gauss_newton):\n",
        "        # J: jacobian matrix not used in the overdetermined case.\n",
        "        # JJ: gauss-newton hessian approximation\n",
        "        # rhs: gradient when overdetermined, residuals when underdetermined.\n",
        "        # outputs: prediction of the model for the current inputs.\n",
        "        J, JJ, rhs, outputs = init_gauss_newton(inputs, targets)\n",
        "\n",
        "        # Perform normalization for numerical stability.\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        normalization_factor = 1.0 / tf.dtypes.cast(\n",
        "            batch_size,\n",
        "            dtype=self.model.dtype)\n",
        "\n",
        "        JJ *= normalization_factor\n",
        "        rhs *= normalization_factor\n",
        "\n",
        "        # Compute the current loss value.\n",
        "        loss = self.loss(targets, outputs)\n",
        "\n",
        "        stop_training = False\n",
        "        attempt = 0\n",
        "        damping_factor = self.damping_algorithm.init_step(\n",
        "            self.damping_factor, loss)\n",
        "\n",
        "        attempts = tf.constant(self.attempts_per_step, dtype=tf.int32)\n",
        "\n",
        "        while tf.constant(True, dtype=tf.bool):\n",
        "            # Apply the damping to the gauss-newton hessian approximation.\n",
        "            JJ_damped = self.damping_algorithm.apply(damping_factor, JJ)\n",
        "\n",
        "            # Compute the updates:\n",
        "            # overdetermined: updates = (J'*J + damping)^-1*J'*residuals\n",
        "            # underdetermined: updates = J'*(J*J' + damping)^-1*residuals\n",
        "            updates = compute_gauss_newton(J, JJ_damped, rhs)\n",
        "\n",
        "            # Split and Reshape the updates\n",
        "            updates = tf.split(tf.squeeze(updates, axis=-1), self._splits)\n",
        "            updates = [tf.reshape(update, shape)\n",
        "                       for update, shape in zip(updates, self._shapes)]\n",
        "\n",
        "            # Apply the updates to the model trainable_variables.\n",
        "            self.optimizer.apply_gradients(\n",
        "                zip(updates, self.model.trainable_variables))\n",
        "\n",
        "            if attempt < attempts:\n",
        "                attempt += 1\n",
        "\n",
        "                # Compute the new loss value.\n",
        "                outputs = self.model(inputs, training=False)\n",
        "                new_loss = self.loss(targets, outputs)\n",
        "\n",
        "                if new_loss < loss:\n",
        "                    # Accept the new model variables and backup them.\n",
        "                    loss = new_loss\n",
        "                    damping_factor = self.damping_algorithm.decrease(\n",
        "                        damping_factor, loss)\n",
        "                    self.backup_variables()\n",
        "                    break\n",
        "\n",
        "                # Restore the old variables and try a new damping_factor.\n",
        "                damping_factor = self.damping_algorithm.increase(\n",
        "                    damping_factor, loss)\n",
        "                self.restore_variables()\n",
        "\n",
        "                stop_training = self.damping_algorithm.stop_training(\n",
        "                    damping_factor, loss)\n",
        "                if stop_training:\n",
        "                    break\n",
        "            else:\n",
        "                break\n",
        "\n",
        "        # Update the damping_factor which will be used in the next train_step.\n",
        "        self.damping_factor.assign(damping_factor)\n",
        "        return loss, outputs, attempt, stop_training\n",
        "\n",
        "    @tf.function\n",
        "    def _compute_num_outputs(self, inputs, targets):\n",
        "        input_shape = inputs.shape[1::]\n",
        "        target_shape = targets.shape[1::]\n",
        "        _inputs = tf.keras.Input(shape=input_shape,\n",
        "                                 dtype=inputs.dtype)\n",
        "        _targets = tf.keras.Input(shape=target_shape,\n",
        "                                  dtype=targets.dtype)\n",
        "        outputs = self.model(_inputs)\n",
        "        residuals = self.loss.residuals(_targets, outputs)\n",
        "        return tf.reduce_prod(residuals.shape[1::])\n",
        "\n",
        "    @tf.function\n",
        "    def reset_damping_factor(self):\n",
        "        self.damping_factor.assign(self.damping_algorithm.starting_value)\n",
        "\n",
        "    @tf.function\n",
        "    def backup_variables(self):\n",
        "        zip_args = (self.model.trainable_variables, self._backup_variables)\n",
        "        for variable, backup in zip(*zip_args):\n",
        "            backup.assign(variable)\n",
        "\n",
        "    @tf.function\n",
        "    def restore_variables(self):\n",
        "        zip_args = (self.model.trainable_variables, self._backup_variables)\n",
        "        for variable, backup in zip(*zip_args):\n",
        "            variable.assign(backup)\n",
        "\n",
        "    @tf.function\n",
        "    def train_step(self, inputs, targets):\n",
        "        if self._num_outputs is None:\n",
        "            self._num_outputs = self._compute_num_outputs(inputs, targets)\n",
        "\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        num_residuals = batch_size * self._num_outputs\n",
        "        overdetermined = num_residuals >= self._num_variables\n",
        "\n",
        "        if overdetermined:\n",
        "            loss, outputs, attempts, stop_training = self._train_step(\n",
        "                inputs,\n",
        "                targets,\n",
        "                self._init_gauss_newton_overdetermined,\n",
        "                self._compute_gauss_newton_overdetermined)\n",
        "        else:\n",
        "            loss, outputs, attempts, stop_training = self._train_step(\n",
        "                inputs,\n",
        "                targets,\n",
        "                self._init_gauss_newton_underdetermined,\n",
        "                self._compute_gauss_newton_underdetermined)\n",
        "\n",
        "        return loss, outputs, attempts, stop_training\n",
        "\n",
        "    def fit(self, dataset, epochs=1, metrics=None):\n",
        "        \"\"\"Trains self.model on the dataset for a fixed number of epochs.\n",
        "\n",
        "        Arguments:\n",
        "            dataset: A `tf.data` dataset, must return a tuple (inputs, targets).\n",
        "            epochs: Integer. Number of epochs to train the model.\n",
        "            metrics: List of metrics to be evaluated during training.\n",
        "        \"\"\"\n",
        "        self.backup_variables()\n",
        "        steps = dataset.cardinality().numpy().item()\n",
        "        stop_training = False\n",
        "\n",
        "        if metrics is None:\n",
        "            metrics = []\n",
        "\n",
        "        pl = tf.keras.callbacks.ProgbarLogger(\n",
        "            count_mode='steps',\n",
        "            stateful_metrics=[\"damping_factor\", \"attempts\"])\n",
        "\n",
        "        pl.set_params(\n",
        "            {\"verbose\": 1, \"epochs\": epochs, \"steps\": steps})\n",
        "\n",
        "        pl.on_train_begin()\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            if stop_training:\n",
        "                break\n",
        "\n",
        "            # Reset metrics.\n",
        "            for m in metrics:\n",
        "                m.reset_states()\n",
        "\n",
        "            pl.on_epoch_begin(epoch)\n",
        "\n",
        "            iterator = iter(dataset)\n",
        "\n",
        "            for step in range(steps):\n",
        "                if stop_training:\n",
        "                    break\n",
        "\n",
        "                pl.on_train_batch_begin(step)\n",
        "\n",
        "                inputs, targets = next(iterator)\n",
        "\n",
        "                loss, outputs, attempts, stop_training = \\\n",
        "                    self.train_step(inputs, targets)\n",
        "\n",
        "                # Update metrics.\n",
        "                for m in metrics:\n",
        "                    m.update_state(targets, outputs)\n",
        "\n",
        "                logs = {\"damping_factor\": self.damping_factor,\n",
        "                        \"attempts\": attempts,\n",
        "                        \"loss\": loss}\n",
        "                logs.update({m.name: m.result() for m in metrics})\n",
        "\n",
        "                pl.on_train_batch_end(step, logs)\n",
        "\n",
        "            pl.on_epoch_end(epoch)\n",
        "\n",
        "        pl.on_train_end()\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "\n",
        "\n",
        "class ModelWrapper(tf.keras.Sequential):\n",
        "    \"\"\"Wraps a keras model.\n",
        "\n",
        "    When fit is called, the wrapped model is trained using Levenberg–Marquardt.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model):\n",
        "        if not model.built:\n",
        "            raise ValueError('This model has not yet been built. '\n",
        "                             'Build the model first by calling `build()` or '\n",
        "                             'calling `fit()` with some data, or specify an '\n",
        "                             '`input_shape` argument in the first layer(s) for '\n",
        "                             'automatic build.')\n",
        "\n",
        "        super(ModelWrapper, self).__init__([model])\n",
        "        self.model = model\n",
        "        self.trainer = None\n",
        "\n",
        "    def compile(self,\n",
        "                optimizer=tf.keras.optimizers.SGD(learning_rate=1.0),\n",
        "                loss=MeanSquaredError(),\n",
        "                damping_algorithm=DampingAlgorithm(),\n",
        "                attempts_per_step=6,\n",
        "                solve_method='qr',\n",
        "                jacobian_max_num_rows=100,\n",
        "                experimental_use_pfor=True,\n",
        "                metrics=None,\n",
        "                loss_weights=None,\n",
        "                weighted_metrics=None,\n",
        "                run_eagerly=None,\n",
        "                **kwargs):\n",
        "        super(ModelWrapper, self).compile(\n",
        "            optimizer=optimizer,\n",
        "            loss=loss,\n",
        "            metrics=metrics,\n",
        "            loss_weights=loss_weights,\n",
        "            weighted_metrics=weighted_metrics,\n",
        "            run_eagerly=run_eagerly)\n",
        "\n",
        "        self.trainer = Trainer(\n",
        "            model=self.model,\n",
        "            optimizer=optimizer,\n",
        "            loss=loss,\n",
        "            damping_algorithm=damping_algorithm,\n",
        "            attempts_per_step=attempts_per_step,\n",
        "            solve_method=solve_method,\n",
        "            jacobian_max_num_rows=jacobian_max_num_rows,\n",
        "            experimental_use_pfor=experimental_use_pfor)\n",
        "\n",
        "    def _assign_stop_training(self, value):\n",
        "        self.stop_training = value.numpy().item()\n",
        "\n",
        "    def train_step(self, data):\n",
        "        inputs, targets = data\n",
        "\n",
        "        loss, outputs, attempts, stop_training = \\\n",
        "            self.trainer.train_step(inputs, targets)\n",
        "\n",
        "        self.compiled_metrics.update_state(targets, outputs)\n",
        "\n",
        "        logs = {\"damping_factor\": self.trainer.damping_factor,\n",
        "                \"attempts\": attempts,\n",
        "                \"loss\": loss}\n",
        "        logs.update({m.name: m.result() for m in self.metrics})\n",
        "\n",
        "        # BUG: In tensorflow v2.2.0 and v2.3.0 setting model.stop_training=True\n",
        "        # does not stop training immediately, but only at the end of the epoch.\n",
        "        # https://github.com/tensorflow/tensorflow/issues/41174\n",
        "        tf.py_function(self._assign_stop_training, [stop_training], Tout=[])\n",
        "\n",
        "        return logs\n",
        "\n",
        "    def fit(self,\n",
        "            x=None,\n",
        "            y=None,\n",
        "            batch_size=None,\n",
        "            epochs=1,\n",
        "            verbose=1,\n",
        "            callbacks=None,\n",
        "            **kwargs):\n",
        "        if verbose > 0:\n",
        "            if callbacks is None:\n",
        "                callbacks = []\n",
        "\n",
        "            callbacks.append(tf.keras.callbacks.ProgbarLogger(\n",
        "                count_mode='steps',\n",
        "                stateful_metrics=[\"damping_factor\", \"attempts\"]))\n",
        "\n",
        "        super(ModelWrapper, self).fit(\n",
        "            x=x,\n",
        "            y=y,\n",
        "            batch_size=batch_size,\n",
        "            epochs=epochs,\n",
        "            verbose=verbose,\n",
        "            callbacks=callbacks,\n",
        "            **kwargs)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unTfqvN2VPoB"
      },
      "source": [
        "# Test curve fitting\n",
        "\n",
        "The function `y = sinc(10 * x)` is fitted using a Shallow Neural Network with 61 parameters.\n",
        "Despite the triviality of the problem, first-order methods such as Adam fail to converge, while Levenberg–Marquardt converges rapidly with very low loss values. The values of learning_rate were chosen experimentally on the basis of the results obtained by each algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLMXJYbEThWC",
        "cellView": "both"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "input_size = 20000\n",
        "batch_size = 1000\n",
        "\n",
        "x_train = np.linspace(-1, 1, input_size, dtype=np.float64)\n",
        "y_train = np.sinc(10 * x_train)\n",
        "\n",
        "x_train = tf.expand_dims(tf.cast(x_train, tf.float32), axis=-1)\n",
        "y_train = tf.expand_dims(tf.cast(y_train, tf.float32), axis=-1)\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "train_dataset = train_dataset.shuffle(input_size)\n",
        "train_dataset = train_dataset.batch(batch_size).cache()\n",
        "train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(20, activation='tanh', input_shape=(1,)),\n",
        "    tf.keras.layers.Dense(1, activation='linear')])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
        "    loss=tf.keras.losses.MeanSquaredError())\n",
        "\n",
        "model_wrapper = ModelWrapper(\n",
        "    tf.keras.models.clone_model(model))\n",
        "\n",
        "model_wrapper.compile(\n",
        "    optimizer=tf.keras.optimizers.SGD(learning_rate=1.0),\n",
        "    loss=MeanSquaredError())\n",
        "\n",
        "print(\"Train using Adam\")\n",
        "t1_start = time.perf_counter()\n",
        "model.fit(train_dataset, epochs=1000)\n",
        "t1_stop = time.perf_counter()\n",
        "print(\"Elapsed time: \", t1_stop - t1_start)\n",
        "\n",
        "print(\"\\n_________________________________________________________________\")\n",
        "print(\"Train using Levenberg-Marquardt\")\n",
        "t2_start = time.perf_counter()\n",
        "model_wrapper.fit(train_dataset, epochs=100)\n",
        "t2_stop = time.perf_counter()\n",
        "print(\"Elapsed time: \", t2_stop - t2_start)\n",
        "\n",
        "print(\"\\n_________________________________________________________________\")\n",
        "print(\"Plot results\")\n",
        "plt.plot(x_train, y_train, 'b-', label=\"reference\")\n",
        "plt.plot(x_train, model.predict(x_train), 'g--', label=\"adam\")\n",
        "plt.plot(x_train, model_wrapper.predict(x_train), 'r--', label=\"lm\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfEHkJ_VVWdf"
      },
      "source": [
        "# Test mnist classification\n",
        "\n",
        "The classification is performed using a Convolutional Neural Network with 1026 parameters.\n",
        "This time there were no particular benefits as in the previous case. Even if Levenberg–Marquardt converges with far fewer epochs than Adam, the longer execution time per step nullifies its advantages.\n",
        "However, both methods achieve roughly the same accuracy values on train and test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnvOIrjSkcEo",
        "cellView": "both"
      },
      "source": [
        "import tensorflow as tf\n",
        "import time\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "x_train = tf.cast(x_train / 255.0, dtype=tf.float32)\n",
        "x_test = tf.cast(x_test / 255.0, dtype=tf.float32)\n",
        "\n",
        "x_train = tf.expand_dims(x_train, axis=-1)\n",
        "x_test = tf.expand_dims(x_test, axis=-1)\n",
        "\n",
        "y_train = tf.cast(y_train, dtype=tf.float32)\n",
        "y_test = tf.cast(y_test, dtype=tf.float32)\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "train_dataset = train_dataset.shuffle(60000)\n",
        "train_dataset = train_dataset.batch(6000).cache()\n",
        "train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(filters=8, kernel_size=4, strides=2, padding='valid',\n",
        "                           activation='tanh', input_shape=(28, 28, 1)),\n",
        "    tf.keras.layers.Conv2D(filters=4, kernel_size=4, strides=2, padding='valid',\n",
        "                           activation='tanh'),\n",
        "    tf.keras.layers.Conv2D(filters=4, kernel_size=2, strides=1, padding='valid',\n",
        "                           activation='tanh'),\n",
        "    tf.keras.layers.Conv2D(filters=4, kernel_size=2, strides=1, padding='valid',\n",
        "                           activation='tanh'),\n",
        "    tf.keras.layers.Conv2D(filters=4, kernel_size=2, strides=1, padding='valid',\n",
        "                           activation='tanh'),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(10, activation='linear')\n",
        "])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[\"accuracy\"])\n",
        "\n",
        "model_wrapper = ModelWrapper(tf.keras.models.clone_model(model))\n",
        "\n",
        "model_wrapper.compile(\n",
        "    optimizer=tf.keras.optimizers.SGD(learning_rate=0.1),\n",
        "    loss=SparseCategoricalCrossentropy(from_logits=True),\n",
        "    solve_method='solve',\n",
        "    metrics=['accuracy'])\n",
        "\n",
        "print(\"Train using Adam\")\n",
        "t1_start = time.perf_counter()\n",
        "model.fit(train_dataset, epochs=200)\n",
        "t1_stop = time.perf_counter()\n",
        "print(\"Elapsed time: \", t1_stop - t1_start)\n",
        "\n",
        "print(\"\\n_________________________________________________________________\")\n",
        "print(\"Train using Levenberg-Marquardt\")\n",
        "t2_start = time.perf_counter()\n",
        "model_wrapper.fit(train_dataset, epochs=10)\n",
        "t2_stop = time.perf_counter()\n",
        "print(\"Elapsed time: \", t2_stop - t2_start)\n",
        "\n",
        "print(\"\\n_________________________________________________________________\")\n",
        "print(\"Test set results\")\n",
        "\n",
        "test_loss, test_acc = model.evaluate(x=x_test, y=y_test, verbose=0)\n",
        "print(\"adam - test_loss: %f - test_accuracy: %f\" % (test_loss, test_acc))\n",
        "\n",
        "test_loss, test_acc = model_wrapper.evaluate(x=x_test, y=y_test, verbose=0)\n",
        "print(\"lm - test_loss: %f - test_accuracy: %f\" % (test_loss, test_acc))\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}